{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2D Multi-Object Search (MOS) + manipulation Task.\n",
    "Uses the domain, models, and agent/environment\n",
    "to actually define the POMDP problem for multi-object search.\n",
    "Then, solve it using POUCT or POMCP\"\"\"\n",
    "import pomdp_py\n",
    "from pomdp_problems.rearrange_pomdp.env.env import *\n",
    "from pomdp_problems.rearrange_pomdp.env.visual import *\n",
    "from pomdp_problems.rearrange_pomdp.agent.agent import *\n",
    "from pomdp_problems.rearrange_pomdp.example_worlds import *\n",
    "from pomdp_problems.rearrange_pomdp.domain.observation import *\n",
    "from pomdp_problems.rearrange_pomdp.models.components.grid_map import *\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "from icecream import ic\n",
    "from pomdp_problems.rearrange_pomdp.manip_problem import ManipOOPOMDP, belief_update,belief_update_manip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(problem):\n",
    "    ic (problem)\n",
    "    robot_id = problem.agent.robot_id\n",
    "    ic (set(problem.env.state.object_states[robot_id].objects_found))\n",
    "    ic (problem.env.target_objects)\n",
    "    ic (len(problem.env.target_objects))\n",
    "\n",
    "def print_info(problem,planner, robot_id, real_action,real_observation,reward, \n",
    "               step,_total_reward,_find_actions_count,_pick_actions_count):\n",
    "    print(\"==== Step %d ====\" % (step+1))\n",
    "    print(\"Action: %s\" % str(real_action))\n",
    "    print(\"Observation: %s\" % str(real_observation))\n",
    "    print(\"Reward: %s\" % str(reward))\n",
    "    print(\"Reward (Cumulative): %s\" % str(_total_reward))\n",
    "    print(\"Find Actions Count: %d\" %  _find_actions_count)\n",
    "    print(\"Pick Actions Count: %d\" % _pick_actions_count)\n",
    "    ic (problem.env.state.object_states[robot_id]['objects_picked'])\n",
    "\n",
    "    if isinstance(planner, pomdp_py.POUCT):\n",
    "        print(\"__num_sims__: %d\" % planner.last_num_sims)\n",
    "\n",
    "def update_action_counts(real_action, _find_actions_count,_pick_actions_count):\n",
    "\n",
    "    if isinstance(real_action, FindAction):\n",
    "        _find_actions_count += 1\n",
    "    if isinstance(real_action,PickAction):\n",
    "        _pick_actions_count += 1\n",
    "\n",
    "    return _find_actions_count, _pick_actions_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(viz, problem, robot_id, real_action, real_observation):\n",
    "    # This is used to show the sensing range; Not sampled\n",
    "    # according to observation model.\n",
    "    robot_pose = problem.env.state.object_states[robot_id].pose\n",
    "    viz_observation = MosOOObservation({})\n",
    "    if isinstance(real_action, LookAction) or isinstance(real_action, FindAction):\n",
    "        viz_observation = \\\n",
    "            problem.env.sensors[robot_id].observe(robot_pose,\n",
    "                                                    problem.env.state)\n",
    "    viz.update(robot_id,\n",
    "                real_action,\n",
    "                real_observation,\n",
    "                viz_observation,\n",
    "                problem.agent.cur_belief)\n",
    "    viz.on_loop()\n",
    "    viz.on_render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termination_condition_check(problem, robot_id, _find_actions_count,\n",
    "                                 _pick_actions_count, _time_used,max_time):\n",
    "\n",
    "    if set(problem.env.state.object_states[robot_id].objects_found)\\\n",
    "        == problem.env.target_objects  and \\\n",
    "        problem.env.state.object_states[robot_id].objects_picked \\\n",
    "        == len(problem.env.target_objects):\n",
    "        ic (problem.env.state.object_states[robot_id].objects_found)\n",
    "        ic (problem.env.state.object_states[robot_id].objects_picked)\n",
    "        ic (len(problem.env.target_objects))\n",
    "        print(\"Done!\")\n",
    "        #break\n",
    "        return True\n",
    "    if _find_actions_count >= len(problem.env.target_objects) \\\n",
    "        and _pick_actions_count >= len(problem.env.target_objects):\n",
    "        print(\"FindAction limit reached, pick actions currently not checked limit limit reached.\")\n",
    "        return True\n",
    "    if _time_used > max_time:\n",
    "        print(\"Maximum time reached.\")\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Solve the problem with POUCT/POMCP planner ###\n",
    "### This is the main online POMDP solver logic ###\n",
    "def solve_init(problem,\n",
    "          max_depth=10,  # planning horizon\n",
    "          discount_factor=0.99,\n",
    "          planning_time=1.,       # amount of time (s) to plan each step\n",
    "          exploration_const=1000, # exploration constant\n",
    "          visualize=True,\n",
    "          max_time=120,  # maximum amount of time allowed to solve the problem\n",
    "          max_steps=500): # maximum number of planning steps the agent can take.\n",
    "    \"\"\"\n",
    "    This function terminates when:\n",
    "    - maximum time (max_time) reached; This time includes planning and updates\n",
    "    - agent has planned `max_steps` number of steps\n",
    "    - agent has taken n FindAction(s) where n = number of target objects.\n",
    "\n",
    "    Args:\n",
    "        visualize (bool) if True, show the pygame visualization.\n",
    "    \"\"\"\n",
    "\n",
    "    random_objid = random.sample(problem.env.target_objects, 1)[0]\n",
    "    random_object_belief = problem.agent.belief.object_beliefs[random_objid]\n",
    "    if isinstance(random_object_belief, pomdp_py.Histogram):\n",
    "        # Use POUCT\n",
    "        planner = pomdp_py.POUCT(max_depth=max_depth,\n",
    "                                 discount_factor=discount_factor,\n",
    "                                 planning_time=planning_time,\n",
    "                                 exploration_const=exploration_const,\n",
    "                                 rollout_policy=problem.agent.policy_model)  # Random by default\n",
    "    elif isinstance(random_object_belief, pomdp_py.Particles):\n",
    "        # Use POMCP\n",
    "        planner = pomdp_py.POMCP(max_depth=max_depth,\n",
    "                                 discount_factor=discount_factor,\n",
    "                                 planning_time=planning_time,\n",
    "                                 exploration_const=exploration_const,\n",
    "                                 rollout_policy=problem.agent.policy_model)  # Random by default\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported object belief type %s\" % str(type(random_object_belief)))\n",
    "\n",
    "    robot_id = problem.agent.robot_id\n",
    "    viz = None\n",
    "    if visualize:\n",
    "        viz = MosViz(problem.env, controllable=False)  # controllable=False means no keyboard control.\n",
    "        if viz.on_init() == False:\n",
    "            raise Exception(\"Environment failed to initialize\")\n",
    "        viz.update(robot_id,\n",
    "                   None,\n",
    "                   None,\n",
    "                   None,\n",
    "                   problem.agent.cur_belief)\n",
    "        viz.on_render()\n",
    "\n",
    "    return planner, robot_id, viz\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "grid_map, robot_char = random_world(10, 10, 5, 10,seed=seed)\n",
    "#ic (grid_map)\n",
    "#exit()\n",
    "laserstr = make_laser_sensor(90, (1, 4), 0.5, False)\n",
    "proxstr = make_proximity_sensor(4, False)\n",
    "problem = ManipOOPOMDP(robot_char,  # r is the robot character\n",
    "                        sigma=0.05,  # observation model parameter\n",
    "                        epsilon=0.95, # observation model parameter\n",
    "                        grid_map=grid_map,\n",
    "                        sensors={robot_char: proxstr},\n",
    "                        prior=\"uniform\",\n",
    "                        agent_has_map=True)\n",
    "max_steps = 5000\n",
    "max_time = 120\n",
    "visualize = False\n",
    "\n",
    "planner, robot_id, viz = solve_init(problem,\n",
    "        max_depth=10,\n",
    "        discount_factor=0.99,\n",
    "        planning_time=1.,\n",
    "        exploration_const=1000,\n",
    "        visualize=visualize,\n",
    "        max_time=120,\n",
    "        max_steps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_time_used = 0\n",
    "_find_actions_count = 0\n",
    "_pick_actions_count = 0\n",
    "_total_reward = 0  # total, undiscounted reward\n",
    "for i in range(max_steps):\n",
    "    # Plan action\n",
    "    _start = time.time()\n",
    "    real_action = planner.plan(problem.agent)\n",
    "    _time_used += time.time() - _start\n",
    "    if _time_used > max_time:\n",
    "        break  # no more time to update.\n",
    "\n",
    "    # Execute action\n",
    "    reward = problem.env.state_transition(real_action, execute=True,\n",
    "                                            robot_id=robot_id)\n",
    "\n",
    "    # Receive observation\n",
    "    _start = time.time()\n",
    "    real_observation = \\\n",
    "        problem.env.provide_observation(problem.agent.observation_model, real_action)\n",
    "\n",
    "    # Updates\n",
    "    problem.agent.clear_history()  # truncate history\n",
    "    problem.agent.update_history(real_action, real_observation)\n",
    "    belief_update_manip(problem.agent, real_action, real_observation,\n",
    "                    problem.env.state.object_states[robot_id],\n",
    "                    planner)\n",
    "    _time_used += time.time() - _start\n",
    "\n",
    "    _total_reward += reward\n",
    "    # Info and render\n",
    "    _find_actions_count, _pick_actions_count = update_action_counts(real_action, _find_actions_count,_pick_actions_count)\n",
    "\n",
    "    print_info(problem, planner,robot_id, real_action,real_observation,reward, i\n",
    "                ,_total_reward, _find_actions_count,_pick_actions_count)\n",
    "\n",
    "    if visualize:\n",
    "        visualize(viz, problem, robot_id, real_action, real_observation)\n",
    "\n",
    "    # Termination check\n",
    "    if termination_condition_check(problem, robot_id, _find_actions_count,\n",
    "                                _pick_actions_count, _time_used,max_time):\n",
    "        break\n",
    "\n",
    "    if _pick_actions_count == 5 :\n",
    "        print (\"Pick actions count reached\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pomdp_py.framework.basics import Action, Agent, POMDP, State, Observation,\\\n",
    "    ObservationModel, TransitionModel, GenerativeDistribution, PolicyModel,\\\n",
    "    sample_generative_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = problem.agent\n",
    "tree = agent.tree\n",
    "history = agent.history\n",
    "state = agent.sample_belief()\n",
    "action = agent.policy_model.rollout(state, history)\n",
    "print (action)\n",
    "next_state, observation, reward, nsteps = sample_generative_model(agent, state, action)\n",
    "print (reward,nsteps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pomdp_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
